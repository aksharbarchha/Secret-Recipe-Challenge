{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating rows of each category item\n",
    "snack = data[data['category']=='snack']\n",
    "condiment = data[data['category']=='condiment']\n",
    "appetizer = data[data['category']=='appetizer']\n",
    "beverage = data[data['category']=='beverage']\n",
    "dessert = data[data['category']=='dessert']\n",
    "\n",
    "#Separating text fields\n",
    "snackt = snack.text.tolist()\n",
    "condimentt = condiment.text.tolist()\n",
    "appetizert = appetizer.text.tolist()\n",
    "beveraget = beverage.text.tolist()\n",
    "dessertt = dessert.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = []\n",
    "sp = \"\"\n",
    "for s in snackt:\n",
    "    si.append((s.split(\":\"))[2])\n",
    "    sp+=s.split(\":\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing unwanted characters\n",
    "sp = sp.replace(\"\\\"PrepMethods\\\"\", \"\")\n",
    "sp = sp.replace(\"\\\"\", \"\")\n",
    "sp = sp.replace(\"]\", \"\")\n",
    "sp = sp.replace(\"[\", \"\")\n",
    "sp = sp.replace(\"\\r\\n\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing ingredients in a list and a string\n",
    "di = []\n",
    "dp = \"\"\n",
    "for s in dessertt:\n",
    "    di.append((s.split(\":\"))[2])\n",
    "    dp+=s.split(\":\")[2]\n",
    "\n",
    "#Removing unwanted characters\n",
    "dp = dp.replace(\"\\\"PrepMethods\\\"\", \"\")\n",
    "dp = dp.replace(\"\\\"\", \"\")\n",
    "dp = dp.replace(\"]\", \"\")\n",
    "dp = dp.replace(\"[\", \"\")\n",
    "dp = dp.replace(\"\\r\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing ingredients in a list and a string\n",
    "ai = []\n",
    "ap = \"\"\n",
    "for s in appetizert:\n",
    "    ai.append((s.split(\":\"))[2])\n",
    "    ap+=s.split(\":\")[2]\n",
    "\n",
    "#Removing unwanted characters\n",
    "ap = ap.replace(\"\\\"PrepMethods\\\"\", \"\")\n",
    "ap = ap.replace(\"\\\"\", \"\")\n",
    "ap = ap.replace(\"]\", \"\")\n",
    "ap = ap.replace(\"[\", \"\")\n",
    "ap = ap.replace(\"\\r\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing ingredients in a list and a string\n",
    "bi = []\n",
    "bp = \"\"\n",
    "for s in beveraget:\n",
    "    bi.append((s.split(\":\"))[2])\n",
    "    bp+=s.split(\":\")[2]\n",
    "\n",
    "#Removing unwanted characters\n",
    "bp = bp.replace(\"\\\"PrepMethods\\\"\", \"\")\n",
    "bp = bp.replace(\"\\\"\", \"\")\n",
    "bp = bp.replace(\"]\", \"\")\n",
    "bp = bp.replace(\"[\", \"\")\n",
    "bp = bp.replace(\"\\r\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing ingredients in a list and a string\n",
    "ci = []\n",
    "cp = \"\"\n",
    "for s in condimentt:\n",
    "    ci.append((s.split(\":\"))[2])\n",
    "    cp+=s.split(\":\")[2]\n",
    "\n",
    "#Removing unwanted characters\n",
    "cp = cp.replace(\"\\\"PrepMethods\\\"\", \"\")\n",
    "cp = cp.replace(\"\\\"\", \"\")\n",
    "cp = cp.replace(\"]\", \"\")\n",
    "cp = cp.replace(\"[\", \"\")\n",
    "cp = cp.replace(\"\\r\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 1175), ('cup', 380), ('cups', 231), ('(', 177), (')', 177), ('cereal', 134), ('teaspoon', 134), ('or', 112), ('butter', 95), ('oz', 89), ('chex™', 88), ('tablespoons', 86), ('sugar', 60), ('corn', 59), ('chocolate', 54), ('dried', 48), ('chips', 45), ('ground', 44), ('margarine', 43), ('chopped', 42), ('salt', 38), ('rice', 36), ('brown', 36), ('vanilla', 35), ('powder', 34), ('tablespoon', 34), ('honey', 33), ('peanuts', 32), ('packed', 32), ('bag', 32)]\n"
     ]
    }
   ],
   "source": [
    "temp = sp\n",
    "from nltk.tokenize import word_tokenize\n",
    "temp_tokens=word_tokenize(temp)\n",
    "#print(temp_tokens)\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for word in temp_tokens:\n",
    "   fdist[word.lower()]+=1\n",
    "#print(fdist)\n",
    "fdist20 = fdist.most_common(30)\n",
    "print(fdist20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cereal', 134), ('butter', 95), ('corn', 59)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "food = wn.synset('food.n.02')\n",
    "dataset = list(set([w for s in food.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "dataset.append('corn')\n",
    "list1=[]\n",
    "for word in fdist20:\n",
    "   if word[0] in dataset:\n",
    "       list1.append(word)\n",
    "       if len(list1)==3:\n",
    "           break\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 1604), ('cup', 392), ('(', 176), (')', 176), ('cups', 167), ('or', 165), ('teaspoon', 163), ('sugar', 148), ('betty', 120), ('crocker™', 119), ('mix', 103), ('butter', 94), ('tablespoons', 85), ('vanilla', 84), ('margarine', 75), ('cream', 75), ('box', 72), ('chocolate', 72), ('softened', 71), ('baking', 70), ('eggs', 69), ('cake', 68), ('gold', 62), ('flour', 59), ('teaspoons', 59), ('water', 58), ('and', 57), ('medal™', 57), ('all-purpose', 56), ('oz', 54)]\n"
     ]
    }
   ],
   "source": [
    "temp = dp\n",
    "#from nltk.tokenize import word_tokenize\n",
    "temp_tokens=word_tokenize(temp)\n",
    "#print(temp_tokens)\n",
    "#from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for word in temp_tokens:\n",
    "   fdist[word.lower()]+=1\n",
    "#print(fdist)\n",
    "fdist20 = fdist.most_common(30)\n",
    "print(fdist20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cereal', 134), ('butter', 95), ('corn', 59)]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "# food = wn.synset('food.n.02')\n",
    "# dataset = list(set([w for s in food.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "dataset.append('cream')\n",
    "list1=[]\n",
    "for word in fdist20:\n",
    "   if word[0] in dataset:\n",
    "       list1.append(word)\n",
    "       if len(list1)==3:\n",
    "           break\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 1240), ('cup', 287), ('(', 193), (')', 193), ('chopped', 185), ('teaspoon', 179), ('tablespoons', 106), ('or', 97), ('tablespoon', 93), ('fresh', 91), ('finely', 75), ('cups', 73), ('sugar', 63), ('juice', 61), ('salt', 58), ('ground', 54), ('pepper', 54), ('oz', 51), ('red', 47), ('oil', 47), ('garlic', 45), ('medium', 44), ('teaspoons', 43), ('and', 41), ('leaves', 39), ('1', 39), ('vinegar', 37), ('packed', 36), ('grated', 34), ('lemon', 33)]\n"
     ]
    }
   ],
   "source": [
    "temp = cp\n",
    "#from nltk.tokenize import word_tokenize\n",
    "temp_tokens=word_tokenize(temp)\n",
    "#print(temp_tokens)\n",
    "#from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for word in temp_tokens:\n",
    "   fdist[word.lower()]+=1\n",
    "#print(fdist)\n",
    "fdist20 = fdist.most_common(30)\n",
    "print(fdist20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pepper', 54), ('garlic', 45), ('leaves', 39)]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "# food = wn.synset('food.n.02')\n",
    "# dataset = list(set([w for s in food.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "dataset.append('garlic')\n",
    "dataset.append('leaves')\n",
    "\n",
    "list1=[]\n",
    "for word in fdist20:\n",
    "   if word[0] in dataset:\n",
    "       list1.append(word)\n",
    "       if len(list1)==3:\n",
    "           break\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 1469), ('(', 328), (')', 328), ('cup', 309), ('chopped', 173), ('teaspoon', 146), ('cheese', 127), ('tablespoons', 123), ('or', 121), ('oz', 93), ('fresh', 91), ('pepper', 76), ('red', 68), ('cut', 63), ('medium', 62), ('sliced', 61), ('finely', 60), ('cups', 58), ('slices', 53), ('sauce', 53)]\n"
     ]
    }
   ],
   "source": [
    "temp = ap\n",
    "#from nltk.tokenize import word_tokenize\n",
    "temp_tokens=word_tokenize(temp)\n",
    "#print(temp_tokens)\n",
    "#from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for word in temp_tokens:\n",
    "   fdist[word.lower()]+=1\n",
    "#print(fdist)\n",
    "fdist20 = fdist.most_common(20)\n",
    "print(fdist20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cheese', 127), ('pepper', 76), ('sauce', 53)]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "# food = wn.synset('food.n.02')\n",
    "# dataset = list(set([w for s in food.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "dataset.append('sauce')\n",
    "dataset.remove('cut')\n",
    "list1=[]\n",
    "for word in fdist20:\n",
    "   if word[0] in dataset:\n",
    "       list1.append(word)\n",
    "       if len(list1)==3:\n",
    "           break\n",
    "print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 1075), ('(', 235), (')', 235), ('cup', 208), ('oz', 122), ('cups', 117), ('juice', 89), ('or', 75), ('frozen', 74), ('desired', 71), ('if', 70), ('tablespoons', 65), ('ice', 65), ('fresh', 62), ('chilled', 50), ('water', 50), ('teaspoon', 50), ('sugar', 48), ('can', 45), ('thawed', 42), ('vanilla', 41), ('milk', 37), ('1', 36), ('concentrate', 35), ('lime', 34), ('tablespoon', 33), ('orange', 33), ('cinnamon', 32), ('cream', 31), ('12', 29)]\n"
     ]
    }
   ],
   "source": [
    "temp = bp\n",
    "#from nltk.tokenize import word_tokenize\n",
    "temp_tokens=word_tokenize(temp)\n",
    "#print(temp_tokens)\n",
    "#from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for word in temp_tokens:\n",
    "   fdist[word.lower()]+=1\n",
    "#print(fdist)\n",
    "fdist20 = fdist.most_common(30)\n",
    "print(fdist20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lime', 34), ('orange', 33), ('cinnamon', 32)]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "# food = wn.synset('food.n.02')\n",
    "# dataset = list(set([w for s in food.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "dataset.append('cinnamon')\n",
    "list1=[]\n",
    "for word in fdist20:\n",
    "   if word[0] in dataset:\n",
    "       list1.append(word)\n",
    "       if len(list1)==3:\n",
    "           break\n",
    "print(list1)\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
